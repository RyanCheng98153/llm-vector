{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "744968d7-bf19-46ba-bc87-c460bf6b646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4be8d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, PreTrainedTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import torch\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcc883ee-f17e-4808-8c22-3fd07f50e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "device = \"cuda\" # \"cuda\" for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=f\"./.cache/{model_name}\")\n",
    "model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=f\"./.cache/{model_name}\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd7d7f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack has a dog, a fluffy golden retriever named Max. Max is a loyal companion, always by Jack's side. Jack has a cat, a sleek black Bengal named Luna. Luna is a gentle soul, and Jack loves her for her playful antics.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Jack has a dog\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "# inputs = tokenizer.encode(prompt).to(device)\n",
    "# print(input_ids)\n",
    "\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=50,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# print(generated_ids)\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "# generated_text = generated_text.replace(prompt, \"\").strip()\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330eaaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(prompt: str, max_new_tokens: int = 50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    # inputs = tokenizer.encode(prompt).to(device)\n",
    "\n",
    "    attention_mask = torch.ones(input_ids.shape,dtype=torch.long,device=device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    # generated_text = generated_text.replace(prompt, \"\").strip()\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97656859",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.serialization.add_safe_globals([DynamicCache])\n",
    "torch.serialization.add_safe_globals([set])\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    past_key_values,\n",
    "    max_new_tokens: int = 20\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text with proper device handling for HuggingFace models using device_map=\"auto\"\n",
    "    \n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        input_ids: Input token ids\n",
    "        past_key_values: Previous KV cache\n",
    "        max_length: Maximum sequence length to generate\n",
    "    \"\"\"\n",
    "    # Get the device of the embedding layer\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "\n",
    "    origin_ids = input_ids\n",
    "    # Move input to the same device as embedding layer\n",
    "    input_ids = input_ids.to(embed_device)\n",
    "    \n",
    "    # Initialize output tensor on embedding device\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "    \n",
    "    # Main generation loop\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass with proper device placement\n",
    "            outputs = model(\n",
    "                input_ids=next_token,  # Only process last token\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            # Get next token prediction (logits will be on the last device)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # Move next token to embedding device for next iteration\n",
    "            next_token = next_token.to(embed_device)\n",
    "            \n",
    "            # Update KV cache\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Append prediction\n",
    "            output_ids = torch.cat([output_ids, next_token], dim=1)\n",
    "            \n",
    "            # Optional: Check for EOS token\n",
    "            #print(next_token.item())\n",
    "            #print(model.config.eos_token_id)\n",
    "            if next_token.item() in model.config.eos_token_id:\n",
    "                break\n",
    "    # return output_ids[:,origin_ids.shape[-1]:]\n",
    "    return output_ids[:,:]\n",
    "\n",
    "\n",
    "def get_kv_cache(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    ") -> DynamicCache:\n",
    "    \"\"\"\n",
    "    Prepare KV cache for a model distributed across multiple GPUs using device_map=\"auto\"\n",
    "    \n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        prompt: Input text to generate KV cache for\n",
    "    \n",
    "    Returns:\n",
    "        DynamicCache: Distributed KV cache\n",
    "    \"\"\"\n",
    "    # Get embedding layer device\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "    \n",
    "    # Encode and move input to embedding device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(embed_device)\n",
    "    \n",
    "    # Initialize dynamic cache\n",
    "    past_key_values = DynamicCache()\n",
    "    \n",
    "    # Generate KV cache with proper device placement\n",
    "    with torch.no_grad():\n",
    "        outputs:CausalLMOutputWithPast = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "    \n",
    "    # The model's device mapping will automatically place each layer's \n",
    "    # KV cache on the correct device\n",
    "    # print(outputs)\n",
    "    return outputs.past_key_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b5c816c-64f2-423d-a797-fee1ba0ec2c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# output = getResponse(prompt, max_new_tokens=50)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode( prompt , return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknowledge_cache\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#knowledge_cache)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "Cell \u001b[1;32mIn[74], line 56\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(model, input_ids, past_key_values, max_new_tokens)\u001b[0m\n\u001b[0;32m     51\u001b[0m         output_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([output_ids, next_token], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# Optional: Check for EOS token\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m#print(next_token.item())\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;66;03m#print(model.config.eos_token_id)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnext_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m:\n\u001b[0;32m     57\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_ids[:,origin_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "prompt = \"Jack has a dog\"\n",
    "knowledge_cache = get_kv_cache(model, tokenizer, prompt)\n",
    "\n",
    "input_ids = tokenizer.encode( prompt , return_tensors=\"pt\" ).to(model.device)\n",
    "output = generate(model, input_ids, knowledge_cache)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True, temperature=None)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5566fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[13.9783,  2.6477,  3.1462,  ...,  6.3939, 10.8440,  4.2600],\n",
      "         [ 5.7300, -2.1732, -2.1956,  ...,  3.6904,  1.7537, -1.9005],\n",
      "         [ 6.8805, -2.7431, -3.0029,  ..., -1.4643,  4.3299, -1.3837],\n",
      "         [ 9.7011, -1.7691,  0.3797,  ...,  0.7390,  4.7757,  0.9799]]]), past_key_values=DynamicCache(), hidden_states=None, attentions=None)\n",
      "DynamicCache()\n"
     ]
    }
   ],
   "source": [
    "kv = get_kv_cache(model, tokenizer, prompt)\n",
    "print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "719ad943-8971-4c78-aad3-56a281995047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 5, 4, 64)\n",
      "torch.Size([1, 5, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# print(kv.key_cache)\n",
    "print(np.shape(kv.key_cache))\n",
    "print(np.shape(kv.key_cache[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21ac688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64])\n",
      "tensor([ 0.9023,  0.9028,  0.5712, -0.9479,  0.0461,  0.3881,  0.6966,  0.1214,\n",
      "         0.4611,  0.0428,  0.4337, -0.2304, -0.3539, -0.3612, -0.0865,  0.5239,\n",
      "         0.5309,  0.5108,  0.1040, -0.8593, -0.7498,  0.4739, -0.3288,  0.1561,\n",
      "         0.1466,  0.0914, -0.1660,  0.1130, -0.4860, -0.1204,  0.2291, -0.1399,\n",
      "        -0.4165,  0.4028, -0.6254,  0.3496,  0.8387,  0.5649, -0.0890,  0.3319,\n",
      "        -0.0305,  0.4899,  0.1726, -0.3607,  0.2789,  0.5966,  0.1437, -0.7063,\n",
      "         0.1566, -0.4533, -0.1549,  0.3641,  0.0422,  0.4065,  0.0651,  0.2237,\n",
      "        -0.1773,  0.3386, -0.1744, -0.1610,  0.2266,  0.1532,  1.6947,  0.1490])\n",
      "torch.Size([4, 64])\n",
      "tensor([-0.0810, -0.0814, -0.1823,  0.3743, -0.2174, -0.0848,  1.6107,  0.5091,\n",
      "         0.2038,  0.9368, -1.5883,  0.2726, -0.5295, -0.5965, -0.1943, -0.9100,\n",
      "        -0.5116,  0.9445,  1.1200,  0.1485,  0.5198, -0.3313, -0.2594, -0.7822,\n",
      "        -0.2948,  0.2163,  0.3134, -0.2262,  0.7887, -0.9766,  1.9020, -2.4390,\n",
      "        -0.2486, -0.1506,  0.2770,  0.1203,  0.4619,  0.5890, -0.2605, -0.1674,\n",
      "        -0.4032,  0.4197,  0.8256, -0.1332, -0.5389, -0.6471,  0.0235, -0.4637,\n",
      "        -0.0818, -1.0133,  0.9194, -0.3724,  0.2731, -0.2502, -0.2020,  0.9388,\n",
      "         0.8104,  0.4139, -0.8082, -0.6108,  1.3387, -1.1032,  1.1282,  0.5566])\n",
      "torch.Size([4, 64])\n",
      "tensor([-0.5923,  1.0467, -0.9038,  0.4610,  0.1123, -1.0341, -0.4915, -0.9174,\n",
      "        -0.4460,  0.5245,  0.7757,  0.5116,  0.2205,  0.3509, -0.6796,  0.0052,\n",
      "        -0.0317, -0.4656,  0.3794,  0.3034,  0.4316, -0.6438,  0.0144, -1.5839,\n",
      "        -1.3614, -0.4200, -0.7020,  0.7596,  0.8648, -0.9470,  1.1534,  1.0665,\n",
      "        -1.6863, -1.6647,  0.7438, -0.8227, -0.9356,  0.7101, -0.8221,  0.4336,\n",
      "         0.1831,  0.7146, -0.6077,  0.1747, -0.3347,  0.5182,  0.3676, -0.2883,\n",
      "        -0.4550,  0.0768,  0.3980,  0.1789, -0.1863, -0.3715,  1.0170, -0.0340,\n",
      "         0.1765, -1.4034, -1.9882,  1.3615,  1.3076, -1.5780, -0.9912,  1.3193])\n",
      "torch.Size([4, 64])\n",
      "tensor([ 0.5800, -1.0162,  0.4150,  1.3811, -0.2400,  0.5574, -1.1942, -0.4492,\n",
      "         0.0259, -0.2834,  0.4778, -0.9336, -0.2839,  1.1747, -0.8224,  0.6293,\n",
      "         0.5452, -0.3168, -0.9652,  0.3965,  0.7686, -1.9069,  0.1780,  0.3074,\n",
      "         0.9304,  0.3105,  0.4659, -0.7156,  0.8392, -1.2711,  0.9823, -1.1126,\n",
      "        -0.3972,  0.1125,  0.8802, -0.4162,  1.1543, -0.8779, -0.0494, -0.8350,\n",
      "        -0.8371,  1.0105,  0.2148, -0.7755, -0.3515,  0.9342, -0.6695, -0.8958,\n",
      "        -0.7903,  0.1645, -0.8381, -0.2368, -0.7606, -0.7356, -1.5816, -0.3337,\n",
      "        -0.0459,  1.4496,  1.1674, -1.3960,  1.0309,  0.8093,  1.1892,  0.8242])\n",
      "torch.Size([4, 64])\n",
      "tensor([ 0.4820, -0.8242, -0.4902,  0.4295,  0.0542,  0.7623, -0.7826,  0.1409,\n",
      "         0.0526,  0.5138,  0.0625, -0.0702, -0.3427,  0.1943, -0.1810,  0.5554,\n",
      "         0.0546, -0.4215,  0.3575, -0.6457,  0.0334, -0.5467, -0.3086,  0.1853,\n",
      "        -0.1841,  0.1717,  0.0547,  0.1444,  0.1627, -0.1906, -0.0780, -0.7181,\n",
      "        -0.7942, -0.5279,  0.9518, -0.9209,  0.9919, -0.0943,  0.1725,  0.5963,\n",
      "        -0.5884, -0.0787, -0.3799,  0.1250, -0.1782,  0.0962,  0.2325, -0.6797,\n",
      "        -0.2162, -0.4505, -0.2976, -0.1712, -0.8273, -0.1447,  0.0992,  0.1638,\n",
      "        -0.0896,  0.0640, -0.1545,  0.2572,  0.4439,  0.1087,  0.1170,  1.3233])\n"
     ]
    }
   ],
   "source": [
    "# print(kv.key_cache[0][0])\n",
    "for k in kv.key_cache[0][0]:\n",
    "    print(np.shape(k))\n",
    "    print(k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d60917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
